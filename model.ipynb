{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9c0ade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ea42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cattle_data_train.csv\")\n",
    "\n",
    "features = data.iloc[:, 1:-1]\n",
    "yields = data.iloc[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c03tkdlkdj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (210000, 36)\n",
      "Cleaned shape: (210000, 20)\n",
      "Removed 16 features\n"
     ]
    }
   ],
   "source": [
    "# Feature Removal and Preprocessing\n",
    "# Based on correlation analysis and data quality issues\n",
    "\n",
    "# Features to remove (15 total):\n",
    "features_to_remove = [\n",
    "    'Feed_Quantity_lb',      # Duplicate of Feed_Quantity_kg (99.99% correlation)\n",
    "    'Cattle_ID',             # Unique identifier, no predictive value\n",
    "    'Rumination_Time_hrs',   # 55% negative values - data quality issue\n",
    "    'HS_Vaccine',            # Very low correlation (0.000034)\n",
    "    'BQ_Vaccine',            # Very low correlation (0.000466)\n",
    "    'BVD_Vaccine',           # Very low correlation (0.000491)\n",
    "    'Brucellosis_Vaccine',   # Very low correlation (0.002089)\n",
    "    'FMD_Vaccine',           # Very low correlation (0.002477)\n",
    "    'Resting_Hours',         # Nearly zero correlation (0.001653)\n",
    "    'Housing_Score',         # Low correlation (0.004) + 3% missing values\n",
    "    'Feeding_Frequency',     # No correlation (0.000380)\n",
    "    'Walking_Distance_km',   # No correlation (0.001538)\n",
    "    'Body_Condition_Score',  # No correlation (0.001647)\n",
    "    'Humidity_percent',      # Very low correlation (0.002153)\n",
    "    'Grazing_Duration_hrs',  # Very low correlation (0.004350)\n",
    "    'Milking_Interval_hrs'   # Very low correlation (0.014734)\n",
    "]\n",
    "\n",
    "# Remove features\n",
    "data_cleaned = data.drop(columns=features_to_remove)\n",
    "\n",
    "print(f\"Original shape: {data.shape}\")\n",
    "print(f\"Cleaned shape: {data_cleaned.shape}\")\n",
    "print(f\"Removed {len(features_to_remove)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "qb4zf4aahh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced Date with Season:\n",
      "  - Season (Winter/Spring/Summer/Fall)\n",
      "\n",
      "Season distribution:\n",
      "Season\n",
      "Fall      52425\n",
      "Spring    53061\n",
      "Summer    52663\n",
      "Winter    51851\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final shape: (210000, 20)\n"
     ]
    }
   ],
   "source": [
    "# Extract Season from Date column\n",
    "# Analysis shows seasons have strong effect on milk yield:\n",
    "#   - Spring: 16.59 L (+6.4% vs average) - BEST season\n",
    "#   - Winter: 16.12 L (+3.4% vs average)\n",
    "#   - Fall:   15.70 L (+0.7% vs average)\n",
    "#   - Summer: 13.94 L (-10.6% vs average) - WORST season (heat stress)\n",
    "#   - Range: 2.65 L difference between best and worst seasons!\n",
    "\n",
    "# Convert Date to datetime\n",
    "data_cleaned['Date'] = pd.to_datetime(data_cleaned['Date'])\n",
    "\n",
    "# Extract month temporarily to create seasons\n",
    "data_cleaned['Month'] = data_cleaned['Date'].dt.month\n",
    "\n",
    "# Create Season feature (meteorological seasons)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:  # 9, 10, 11\n",
    "        return 'Fall'\n",
    "\n",
    "data_cleaned['Season'] = data_cleaned['Month'].apply(get_season)\n",
    "\n",
    "# Drop both Date and Month (we only keep Season)\n",
    "data_cleaned = data_cleaned.drop(columns=['Date', 'Month'])\n",
    "\n",
    "print(\"Replaced Date with Season:\")\n",
    "print(\"  - Season (Winter/Spring/Summer/Fall)\")\n",
    "print(f\"\\nSeason distribution:\")\n",
    "print(data_cleaned['Season'].value_counts().sort_index())\n",
    "print(f\"\\nFinal shape: {data_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced80a13fbi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (210000, 19)\n",
      "Target shape: (210000,)\n"
     ]
    }
   ],
   "source": [
    "# Update features and target using cleaned data\n",
    "features = data_cleaned.drop(columns=['Milk_Yield_L'])\n",
    "yields = data_cleaned['Milk_Yield_L']\n",
    "\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"Target shape: {yields.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97qzbrdayq",
   "metadata": {},
   "source": [
    "## Data Quality Analysis\n",
    "\n",
    "After feature selection, we performed a comprehensive quality check on all 19 remaining features plus the target variable. This analysis examined:\n",
    "\n",
    "**For Categorical Features:**\n",
    "- Missing values\n",
    "- Unique value counts\n",
    "- Whitespace issues (leading/trailing spaces)\n",
    "- Potential typos or duplicate values\n",
    "\n",
    "**For Numeric Features:**\n",
    "- Missing values  \n",
    "- Range and distribution (min, max, mean, median, std dev)\n",
    "- Impossible values (e.g., negative quantities)\n",
    "- Outliers (using IQR method)\n",
    "\n",
    "### Issues Discovered:\n",
    "\n",
    "**1. Breed Column - Data Entry Errors (CRITICAL)**\n",
    "- **Typo**: \"Holstien\" (112 records) should be \"Holstein\"\n",
    "- **Whitespace**: \" Brown Swiss\" (57 records with leading space)\n",
    "- **Whitespace**: \"Brown Swiss \" (46 records with trailing space)\n",
    "- **Impact**: Model would treat these as 7 different breeds instead of 4!\n",
    "- **Fix**: Strip whitespace and correct typo\n",
    "\n",
    "**2. Milk_Yield_L (Target) - Impossible Values**\n",
    "- **Issue**: 74 records (0.04%) have negative milk yields\n",
    "- **Range**: -5.70 to 44.56 liters\n",
    "- **Why impossible**: Cows cannot produce negative milk!\n",
    "- **Likely cause**: Data entry error or measurement issue\n",
    "- **Fix**: Remove these 74 records\n",
    "\n",
    "**3. Feed_Quantity_kg - Missing Values**\n",
    "- **Issue**: 10,481 records (4.99%) missing feed quantity\n",
    "- **Impact**: Cannot use these records without imputation\n",
    "- **Fix**: Impute with median by Feed_Type (different feed types have different typical quantities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "akrrwea66l",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "Breed\n",
      "Holstein        104775\n",
      "Jersey           42183\n",
      "Guernsey         31672\n",
      "Brown Swiss      31155\n",
      "Holstien           112\n",
      " Brown Swiss        57\n",
      "Brown Swiss         46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique breeds: 7\n",
      "\n",
      "======================================================================\n",
      "After cleaning:\n",
      "Breed\n",
      "Holstein       104887\n",
      "Jersey          42183\n",
      "Guernsey        31672\n",
      "Brown Swiss     31258\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique breeds: 4\n",
      "\n",
      "Breed column cleaned: 7 values -> 4 correct breeds\n"
     ]
    }
   ],
   "source": [
    "# Fix Breed column: Remove whitespace and correct typo\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(data_cleaned['Breed'].value_counts())\n",
    "print(f\"\\nUnique breeds: {data_cleaned['Breed'].nunique()}\")\n",
    "\n",
    "# Step 1: Strip leading/trailing whitespace\n",
    "data_cleaned['Breed'] = data_cleaned['Breed'].str.strip()\n",
    "\n",
    "# Step 2: Fix the typo \"Holstien\" -> \"Holstein\"\n",
    "data_cleaned['Breed'] = data_cleaned['Breed'].replace({'Holstien': 'Holstein'})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"After cleaning:\")\n",
    "print(data_cleaned['Breed'].value_counts())\n",
    "print(f\"\\nUnique breeds: {data_cleaned['Breed'].nunique()}\")\n",
    "print(\"\\nBreed column cleaned: 7 values -> 4 correct breeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ji2kevbx2d",
   "metadata": {},
   "source": [
    "## Handling Impossible Values\n",
    "\n",
    "### Removing Negative Milk Yields\n",
    "\n",
    "Our analysis found **74 records (0.04%)** with negative milk yields, ranging from -5.70 to -0.001 liters. \n",
    "\n",
    "**Why this is impossible:**\n",
    "- Cows cannot physically produce negative milk\n",
    "- This is clearly a data entry or measurement error\n",
    "\n",
    "**Decision: Remove these records**\n",
    "\n",
    "**Justification:**\n",
    "- Only 0.04% of data (minimal impact on training)\n",
    "- Including them would teach the model incorrect patterns\n",
    "- Better to have clean, accurate data than preserve bad records\n",
    "- 209,926 remaining samples is still more than sufficient for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9jrxf1sxrjt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 210,000 records\n",
      "\n",
      "Negative milk yields found: 74 records\n",
      "Range of negative values: -5.700 to -0.015 L\n",
      "\n",
      "After removal: 209,926 records\n",
      "Records removed: 74 (0.035%)\n",
      "\n",
      "All milk yields are now >= 0\n"
     ]
    }
   ],
   "source": [
    "# Remove records with negative milk yields\n",
    "\n",
    "print(f\"Original dataset size: {len(data_cleaned):,} records\")\n",
    "\n",
    "# Check for negative yields\n",
    "negative_yields = data_cleaned[data_cleaned['Milk_Yield_L'] < 0]\n",
    "print(f\"\\nNegative milk yields found: {len(negative_yields)} records\")\n",
    "print(f\"Range of negative values: {negative_yields['Milk_Yield_L'].min():.3f} to {negative_yields['Milk_Yield_L'].max():.3f} L\")\n",
    "\n",
    "# Remove negative yields\n",
    "data_cleaned = data_cleaned[data_cleaned['Milk_Yield_L'] >= 0].copy()\n",
    "\n",
    "print(f\"\\nAfter removal: {len(data_cleaned):,} records\")\n",
    "print(f\"Records removed: {len(negative_yields)} ({len(negative_yields)/210000*100:.3f}%)\")\n",
    "print(\"\\nAll milk yields are now >= 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowfcbh5e8",
   "metadata": {},
   "source": [
    "## Missing Value Imputation\n",
    "\n",
    "### Feed_Quantity_kg - Strategic Imputation\n",
    "\n",
    "**Issue**: 10,481 records (4.99%) are missing Feed_Quantity_kg values.\n",
    "\n",
    "**Why not drop these records?**\n",
    "- Losing 5% of our training data would reduce model performance\n",
    "- The missingness appears random (not systematic)\n",
    "- We have enough information to make reasonable estimates\n",
    "\n",
    "**Imputation Strategy: Median by Feed_Type**\n",
    "\n",
    "We'll impute missing values using the **median** Feed_Quantity_kg for each Feed_Type group.\n",
    "\n",
    "**Rationale:**\n",
    "1. **Different feed types have different quantities**: \n",
    "   - Concentrates: Dense, high-calorie (typically less volume)\n",
    "   - Pasture Grass: Low density (typically more volume)\n",
    "   - Median is better than mean (robust to outliers)\n",
    "\n",
    "2. **Preserves realistic patterns**:\n",
    "   - A cow eating \"Concentrates\" gets the typical concentrate amount\n",
    "   - A cow eating \"Hay\" gets the typical hay amount\n",
    "\n",
    "3. **Maintains group-specific distributions**:\n",
    "   - Doesn't assume all feed types are equal\n",
    "   - Respects domain knowledge about feeding practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7hqy98kz4s8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before imputation:\n",
      "Missing Feed_Quantity_kg: 0 records\n",
      "\n",
      "Median Feed_Quantity_kg by Feed_Type:\n",
      "  Concentrates        : 12.04 kg\n",
      "  Hay                 : 12.03 kg\n",
      "  Mixed_Feed          : 12.01 kg\n",
      "  Dry_Fodder          : 12.00 kg\n",
      "  Pasture_Grass       : 12.00 kg\n",
      "  Crop_Residues       : 11.99 kg\n",
      "  Green_Fodder        : 11.98 kg\n",
      "  Silage              : 11.97 kg\n",
      "\n",
      "======================================================================\n",
      "After imputation:\n",
      "Missing Feed_Quantity_kg: 0 records\n",
      "\n",
      "All Feed_Quantity_kg values imputed successfully\n"
     ]
    }
   ],
   "source": [
    "# Impute missing Feed_Quantity_kg values using median by Feed_Type\n",
    "\n",
    "print(\"Before imputation:\")\n",
    "print(f\"Missing Feed_Quantity_kg: {data_cleaned['Feed_Quantity_kg'].isnull().sum()} records\")\n",
    "\n",
    "# Show median feed quantity for each feed type\n",
    "print(\"\\nMedian Feed_Quantity_kg by Feed_Type:\")\n",
    "feed_medians = data_cleaned.groupby('Feed_Type')['Feed_Quantity_kg'].median().sort_values(ascending=False)\n",
    "for feed_type, median in feed_medians.items():\n",
    "    print(f\"  {feed_type:20s}: {median:.2f} kg\")\n",
    "\n",
    "# Impute missing values with group median\n",
    "data_cleaned['Feed_Quantity_kg'] = data_cleaned.groupby('Feed_Type')['Feed_Quantity_kg'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"After imputation:\")\n",
    "print(f\"Missing Feed_Quantity_kg: {data_cleaned['Feed_Quantity_kg'].isnull().sum()} records\")\n",
    "print(\"\\nAll Feed_Quantity_kg values imputed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2j0e0x01jp",
   "metadata": {},
   "source": [
    "## Train/Test Split Strategy\n",
    "\n",
    "\n",
    "**IMPORTANT**: We perform train/test split **BEFORE** any scaling or normalization to prevent **data leakage**.\n",
    "\n",
    "### What is Data Leakage?\n",
    "\n",
    "Data leakage occurs when information from the test set \"leaks\" into the training process, causing inflated performance estimates that don't generalize to truly unseen data.\n",
    "\n",
    "\n",
    "**Problem**: The scaler calculated mean/std using test data, so the model indirectly knows about test data!\n",
    "\n",
    "### Correct Approach (no leakage):\n",
    "\n",
    "```python\n",
    "# 1. Split first (test data becomes invisible)\n",
    "X_train, X_test = train_test_split(X_all_data)\n",
    "\n",
    "# 2. Fit scaler ONLY on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Only learns from training data\n",
    "\n",
    "# 3. Transform both using training statistics\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Uses training mean/std\n",
    "```\n",
    "\n",
    "**Why this matters**: \n",
    "- Simulates real production scenario (we won't have test data statistics)\n",
    "- Prevents optimistically biased performance estimates\n",
    "- Ensures model truly generalizes to unseen data\n",
    "\n",
    "### Our Split:\n",
    "- **80% Training** (167,940 samples)\n",
    "- **20% Testing** (41,986 samples)\n",
    "- **Random State**: 42 (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5zk9kjt2nki",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cleaned dataset: 209,926 records, 19 features\n",
      "\n",
      "Training set: 167,940 records (80.0%)\n",
      "Test set:     41,986 records (20.0%)\n",
      "\n",
      "Train/test split complete (split BEFORE any scaling to prevent data leakage)\n"
     ]
    }
   ],
   "source": [
    "# Perform Train/Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and target AFTER all data cleaning\n",
    "X = data_cleaned.drop(columns=['Milk_Yield_L'])\n",
    "y = data_cleaned['Milk_Yield_L']\n",
    "\n",
    "print(f\"Total cleaned dataset: {len(X):,} records, {X.shape[1]} features\")\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train):,} records ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:     {len(X_test):,} records ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTrain/test split complete (split BEFORE any scaling to prevent data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7o50ic2ss",
   "metadata": {},
   "source": [
    "## Categorical Encoding Strategy\n",
    "\n",
    "We have 7 categorical features that need encoding before modeling. Our strategy:\n",
    "\n",
    "### Analysis of Train vs Test Sets\n",
    "\n",
    "We verified that **100% of categorical values in test set exist in training set**:\n",
    "- Farm_ID: 1000/1000 farms overlap (100%)\n",
    "- Breed: 7/7 breeds overlap\n",
    "- Climate_Zone: 6/6 zones overlap  \n",
    "- Management_System: 5/5 systems overlap\n",
    "- Lactation_Stage: 3/3 stages overlap\n",
    "- Feed_Type: 8/8 feed types overlap\n",
    "\n",
    "This perfect overlap means our encoding will work seamlessly on test data!\n",
    "\n",
    "### Encoding Approach:\n",
    "\n",
    "**1. Farm_ID (1000 unique values) -> Target Encoding**\n",
    "- **Why**: High cardinality (1000 farms)\n",
    "- **Method**: Replace each farm with its mean milk yield from training data\n",
    "- **Benefit**: Captures farm-specific patterns in just 1 column (vs 999 with one-hot)\n",
    "- **Impact**: Explains 0.46% of variance\n",
    "- **Safe**: 100% overlap with test set, no unseen farms\n",
    "\n",
    "**2. All Other Categoricals -> One-Hot Encoding**\n",
    "- **Breed** (4 values) -> 3 binary columns\n",
    "- **Climate_Zone** (6 values) -> 5 binary columns\n",
    "- **Management_System** (5 values) -> 4 binary columns\n",
    "- **Lactation_Stage** (3 values) -> 2 binary columns ****** (explains 1.37% variance!)\n",
    "- **Feed_Type** (8 values) -> 7 binary columns\n",
    "- **Season** (4 values) -> 3 binary columns ************ (explains 4.66% variance!)\n",
    "\n",
    "**Total**: 1 (Farm_ID) + 24 (one-hot) + 12 (numeric) = **37 features**\n",
    "\n",
    "**Why One-Hot?**\n",
    "- Interpretable (each category has clear coefficient)\n",
    "- No ordinal assumptions (categories not ordered)\n",
    "- Works well for linear and tree-based models\n",
    "- Cardinality manageable (largest is 8 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2uevaaxhw0n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding Farm_ID...\n",
      "Before: Farm_ID has 1000 unique values\n",
      "\n",
      "Farm target encoding statistics:\n",
      "  Mean of farm means: 15.593 L\n",
      "  Std of farm means:  0.414 L\n",
      "  Range: 14.221 to 16.984 L\n",
      "\n",
      "All test farms seen in training (0 unseen farms)\n",
      "\n",
      "After: Farm_ID replaced with Farm_ID_encoded (1 numeric column)\n",
      "Train shape: (167940, 19)\n",
      "Test shape:  (41986, 19)\n"
     ]
    }
   ],
   "source": [
    "# Target Encode Farm_ID using training data statistics only\n",
    "\n",
    "print(\"Target Encoding Farm_ID...\")\n",
    "print(f\"Before: Farm_ID has {X_train['Farm_ID'].nunique()} unique values\\n\")\n",
    "\n",
    "# Calculate mean milk yield per farm from TRAINING data only\n",
    "# Group by Farm_ID in X_train and get corresponding y_train values\n",
    "farm_yield_train = pd.DataFrame({'Farm_ID': X_train['Farm_ID'], 'Milk_Yield': y_train})\n",
    "farm_means = farm_yield_train.groupby('Farm_ID')['Milk_Yield'].mean()\n",
    "\n",
    "print(f\"Farm target encoding statistics:\")\n",
    "print(f\"  Mean of farm means: {farm_means.mean():.3f} L\")\n",
    "print(f\"  Std of farm means:  {farm_means.std():.3f} L\")\n",
    "print(f\"  Range: {farm_means.min():.3f} to {farm_means.max():.3f} L\")\n",
    "\n",
    "# Encode training set\n",
    "X_train['Farm_ID_encoded'] = X_train['Farm_ID'].map(farm_means)\n",
    "\n",
    "# Encode test set using TRAINING statistics (prevent data leakage!)\n",
    "X_test['Farm_ID_encoded'] = X_test['Farm_ID'].map(farm_means)\n",
    "\n",
    "# Check for any unseen farms in test (should be 0 based on our analysis)\n",
    "unseen_farms = X_test['Farm_ID_encoded'].isnull().sum()\n",
    "if unseen_farms > 0:\n",
    "    print(f\"\\nWARNING: {unseen_farms} unseen farms in test set, filling with global mean\")\n",
    "    X_test['Farm_ID_encoded'].fillna(farm_means.mean(), inplace=True)\n",
    "else:\n",
    "    print(f\"\\nAll test farms seen in training (0 unseen farms)\")\n",
    "\n",
    "# Drop original Farm_ID column\n",
    "X_train = X_train.drop(columns=['Farm_ID'])\n",
    "X_test = X_test.drop(columns=['Farm_ID'])\n",
    "\n",
    "print(f\"\\nAfter: Farm_ID replaced with Farm_ID_encoded (1 numeric column)\")\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Test shape:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ez74z3s6s5g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding Categorical Features...\n",
      "Before encoding: (167940, 19)\n",
      "\n",
      "Encoding 6 categorical features:\n",
      "  Breed               : 4 values -> 3 binary columns\n",
      "  Climate_Zone        : 6 values -> 5 binary columns\n",
      "  Management_System   : 5 values -> 4 binary columns\n",
      "  Lactation_Stage     : 3 values -> 2 binary columns\n",
      "  Feed_Type           : 8 values -> 7 binary columns\n",
      "  Season              : 4 values -> 3 binary columns\n",
      "\n",
      "After encoding:\n",
      "  Train: (167940, 37)\n",
      "  Test:  (41986, 37)\n",
      "  Columns match: True\n",
      "\n",
      "Categorical encoding complete!\n",
      "   Total features: 37 (all numeric now)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encode remaining categorical features\n",
    "\n",
    "print(\"One-Hot Encoding Categorical Features...\")\n",
    "print(f\"Before encoding: {X_train.shape}\")\n",
    "\n",
    "# Define categorical columns to encode\n",
    "categorical_cols = ['Breed', 'Climate_Zone', 'Management_System', \n",
    "                   'Lactation_Stage', 'Feed_Type', 'Season']\n",
    "\n",
    "print(f\"\\nEncoding {len(categorical_cols)} categorical features:\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = X_train[col].nunique()\n",
    "    print(f\"  {col:20s}: {n_unique} values -> {n_unique-1} binary columns\")\n",
    "\n",
    "# One-Hot encode (drop_first=True to avoid multicollinearity)\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Ensure test set has same columns as train (in same order)\n",
    "# This handles any edge case where a category might not appear in test\n",
    "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)\n",
    "\n",
    "print(f\"\\nAfter encoding:\")\n",
    "print(f\"  Train: {X_train_encoded.shape}\")\n",
    "print(f\"  Test:  {X_test_encoded.shape}\")\n",
    "print(f\"  Columns match: {list(X_train_encoded.columns) == list(X_test_encoded.columns)}\")\n",
    "\n",
    "print(f\"\\nCategorical encoding complete!\")\n",
    "print(f\"   Total features: {X_train_encoded.shape[1]} (all numeric now)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8day1fd1wu",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Our numeric features have vastly different scales:\n",
    "- Age_Months: [24, 143]\n",
    "- Weight_kg: [250, 750]  \n",
    "- Parity: [1, 6]\n",
    "- Water_Intake_L: [14, 150]\n",
    "\n",
    "### Why Scale?\n",
    "\n",
    "**Models that NEED scaling:**\n",
    "- Neural Networks (MLPRegressor) - gradients unstable without scaling\n",
    "- Linear models with regularization (Ridge, Lasso) - penalties affect large-scale features more\n",
    "- Support Vector Regression - distance-based\n",
    "- K-Nearest Neighbors - distance-based\n",
    "\n",
    "**Models that DON'T need scaling:**\n",
    "- Tree-based: Random Forest, XGBoost, LightGBM, CatBoost\n",
    "- Make decisions on thresholds, not distances\n",
    "\n",
    "### Our Decision: Scale Everything\n",
    "\n",
    "**Why?**\n",
    "- Keeps options open for ALL model types\n",
    "- No downside (tree models unaffected)\n",
    "- Helps convergence for neural networks\n",
    "- Makes coefficients interpretable for linear models\n",
    "\n",
    "### Method: StandardScaler (Z-score normalization)\n",
    "\n",
    "Transforms each feature to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "\n",
    "Formula: `z = (x - mean) / std`\n",
    "\n",
    "**Critical**: Fit scaler on training data ONLY, then transform both train and test using training statistics. This prevents data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rhx2549wpbn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling features with StandardScaler...\n",
      "Features to scale: 37\n",
      "\n",
      "Scaling statistics from training data:\n",
      "  Example feature means: [ 83.450524   500.02723056   3.49977968 182.13083244  12.01271132]\n",
      "  Example feature stds:  [ 34.60915767 144.65626669   1.70685556 105.1192756    3.86423676]\n",
      "\n",
      "After scaling:\n",
      "  Train shape: (167940, 37)\n",
      "  Test shape:  (41986, 37)\n",
      "\n",
      "  Train data now has:\n",
      "    Mean ≈ 0: 0.000000\n",
      "    Std ≈ 1:  1.000003\n",
      "\n",
      "Feature scaling complete!\n"
     ]
    }
   ],
   "source": [
    "# Apply StandardScaler to all features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Scaling features with StandardScaler...\")\n",
    "print(f\"Features to scale: {X_train_encoded.shape[1]}\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on TRAINING data only\n",
    "scaler.fit(X_train_encoded)\n",
    "\n",
    "# Transform both train and test using TRAINING statistics\n",
    "X_train_scaled = scaler.transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# Convert back to DataFrame for interpretability (optional but helpful)\n",
    "X_train_final = pd.DataFrame(X_train_scaled, columns=X_train_encoded.columns, index=X_train_encoded.index)\n",
    "X_test_final = pd.DataFrame(X_test_scaled, columns=X_test_encoded.columns, index=X_test_encoded.index)\n",
    "\n",
    "print(f\"\\nScaling statistics from training data:\")\n",
    "print(f\"  Example feature means: {scaler.mean_[:5]}\")\n",
    "print(f\"  Example feature stds:  {scaler.scale_[:5]}\")\n",
    "\n",
    "print(f\"\\nAfter scaling:\")\n",
    "print(f\"  Train shape: {X_train_final.shape}\")\n",
    "print(f\"  Test shape:  {X_test_final.shape}\")\n",
    "print(f\"\\n  Train data now has:\")\n",
    "print(f\"    Mean ≈ 0: {X_train_final.mean().mean():.6f}\")\n",
    "print(f\"    Std ≈ 1:  {X_train_final.std().mean():.6f}\")\n",
    "\n",
    "print(f\"\\nFeature scaling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xuf655ytwg",
   "metadata": {},
   "source": [
    "## Data Preprocessing Complete!\n",
    "\n",
    "### Final Dataset Summary\n",
    "\n",
    "Our preprocessing pipeline has successfully:\n",
    "1. Removed 16 low-value features\n",
    "2. Extracted Season from Date (explains 4.66% variance)\n",
    "3. Fixed data quality issues (Breed typos, negative yields, missing values)\n",
    "4. Target encoded Farm_ID (1 column, no unseen farms)\n",
    "5. One-Hot encoded 6 categorical features (24 binary columns)\n",
    "6. Scaled all numeric features (mean≈0, std≈1)\n",
    "\n",
    "Let's verify everything is ready for modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxi9fuc5r6j",
   "metadata": {},
   "source": [
    "## Summary of Feature Selection\n",
    "\n",
    "**Removed 16 features:**\n",
    "1. Feed_Quantity_lb - duplicate of Feed_Quantity_kg (99.99% correlation)\n",
    "2. Cattle_ID - unique identifier, no predictive value\n",
    "3. Rumination_Time_hrs - data quality issue (55% negative values)\n",
    "4-8. Low-correlation vaccines: HS, BQ, BVD, Brucellosis, FMD\n",
    "9-15. Zero/near-zero correlation: Resting_Hours, Housing_Score, Feeding_Frequency, Walking_Distance_km, Body_Condition_Score, Humidity_percent, Grazing_Duration_hrs\n",
    "16. Milking_Interval_hrs - very low correlation (0.015)\n",
    "\n",
    "**Replaced Date with Season:**\n",
    "- Removed: Date (raw timestamp)\n",
    "- Added: Season (Winter/Spring/Summer/Fall)\n",
    "- Rationale: Strong seasonal effect on milk yield (Spring: 16.59L vs Summer: 13.94L = 2.65L range)\n",
    "- Month was NOT kept (redundant with Season - only 0.1L variation within seasons)\n",
    "\n",
    "**Final: 19 features (down from 35 = 46% reduction)**\n",
    "\n",
    "**Categorical (7):**\n",
    "- Breed, Climate_Zone, Management_System, Lactation_Stage, Feed_Type, Farm_ID, Season\n",
    "\n",
    "**Numeric (12):**\n",
    "- Age_Months (corr: 0.31), Weight_kg (0.30), Parity (0.24), Days_in_Milk (0.06), Feed_Quantity_kg (0.22), Water_Intake_L (0.12), Ambient_Temperature_C (0.04), Anthrax_Vaccine (0.07), IBR_Vaccine (0.07), Rabies_Vaccine (0.07), Previous_Week_Avg_Yield (0.09), Mastitis (0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a304fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Breed', 'Climate_Zone', 'Management_System', 'Lactation_Stage', 'Feed_Type', 'Farm_ID', 'Season']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Update categorical columns from cleaned data\n",
    "cat_cols = features.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "print(f\"Categorical columns: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4738e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=None, mode=\"freq\", m=5):\n",
    "        self.cols = cols\n",
    "        self.mode = mode\n",
    "        self.m = m\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.maps = {}\n",
    "\n",
    "        for col in self.cols:\n",
    "            freq = X[col].value_counts()\n",
    "            total = len(X)\n",
    "\n",
    "            if self.mode == \"freq\":\n",
    "                enc = freq / total\n",
    "            elif self.mode == \"count\":\n",
    "                enc = freq\n",
    "            elif self.mode == \"logfreq\":\n",
    "                enc = np.log1p(freq / total)\n",
    "            elif self.mode == \"smooth\":\n",
    "                prior = freq.sum() / total\n",
    "                enc = (freq + self.m * prior) / (freq.sum() + self.m)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown mode: \" + self.mode)\n",
    "\n",
    "            self.maps[col] = enc\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].map(self.maps[col]).fillna(0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57dcca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL PREPROCESSING VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Dataset shapes:\n",
      "  X_train_final: (167940, 37)\n",
      "  X_test_final:  (41986, 37)\n",
      "  y_train:       (167940,)\n",
      "  y_test:        (41986,)\n",
      "\n",
      "Data quality checks:\n",
      "  Missing values (train): 0\n",
      "  Missing values (test):  0\n",
      "  Infinite values (train): 0\n",
      "  Infinite values (test):  0\n",
      "\n",
      "Feature statistics:\n",
      "  Total features: 37\n",
      "  Feature names (first 10): ['Age_Months', 'Weight_kg', 'Parity', 'Days_in_Milk', 'Feed_Quantity_kg', 'Water_Intake_L', 'Ambient_Temperature_C', 'Anthrax_Vaccine', 'IBR_Vaccine', 'Rabies_Vaccine']\n",
      "  Feature names (last 5):   ['Feed_Type_Pasture_Grass', 'Feed_Type_Silage', 'Season_Spring', 'Season_Summer', 'Season_Winter']\n",
      "\n",
      "Target statistics:\n",
      "  y_train mean: 15.593 L\n",
      "  y_train std:  5.340 L\n",
      "  y_train range: [0.055, 44.536] L\n",
      "  y_test mean:  15.603 L\n",
      "  y_test std:   5.358 L\n",
      "\n",
      "Scaling verification (should be ~0 mean, ~1 std):\n",
      "  X_train mean: 0.000000\n",
      "  X_train std:  1.000003\n",
      "  X_test mean:  -0.000018\n",
      "  X_test std:   1.000560\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE - READY FOR MODELING!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Train baseline model (Ridge Regression)\n",
      "  2. Try advanced models (LightGBM, CatBoost, Random Forest)\n",
      "  3. Hyperparameter tuning\n",
      "  4. Final model selection and predictions\n"
     ]
    }
   ],
   "source": [
    "# Final verification of preprocessed data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL PREPROCESSING VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  X_train_final: {X_train_final.shape}\")\n",
    "print(f\"  X_test_final:  {X_test_final.shape}\")\n",
    "print(f\"  y_train:       {y_train.shape}\")\n",
    "print(f\"  y_test:        {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"  Missing values (train): {X_train_final.isnull().sum().sum()}\")\n",
    "print(f\"  Missing values (test):  {X_test_final.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values (train): {np.isinf(X_train_final).sum().sum()}\")\n",
    "print(f\"  Infinite values (test):  {np.isinf(X_test_final).sum().sum()}\")\n",
    "\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(f\"  Total features: {X_train_final.shape[1]}\")\n",
    "print(f\"  Feature names (first 10): {list(X_train_final.columns[:10])}\")\n",
    "print(f\"  Feature names (last 5):   {list(X_train_final.columns[-5:])}\")\n",
    "\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"  y_train mean: {y_train.mean():.3f} L\")\n",
    "print(f\"  y_train std:  {y_train.std():.3f} L\")\n",
    "print(f\"  y_train range: [{y_train.min():.3f}, {y_train.max():.3f}] L\")\n",
    "print(f\"  y_test mean:  {y_test.mean():.3f} L\")\n",
    "print(f\"  y_test std:   {y_test.std():.3f} L\")\n",
    "\n",
    "print(f\"\\nScaling verification (should be ~0 mean, ~1 std):\")\n",
    "print(f\"  X_train mean: {X_train_final.mean().mean():.6f}\")\n",
    "print(f\"  X_train std:  {X_train_final.std().mean():.6f}\")\n",
    "print(f\"  X_test mean:  {X_test_final.mean().mean():.6f}\")\n",
    "print(f\"  X_test std:   {X_test_final.std().mean():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE - READY FOR MODELING!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Train baseline model (Ridge Regression)\")\n",
    "print(\"  2. Try advanced models (LightGBM, CatBoost, Random Forest)\")\n",
    "print(\"  3. Hyperparameter tuning\")\n",
    "print(\"  4. Final model selection and predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
